@startuml
left to right direction

package "Docker Compose Setup 2: EDA" as eda {
    folder "docker-compose.yaml" as eda_compose {
        [docker-compose EDA]
        [Dockerfile EDA]
    }

    node "Exploratory Data Analysis Container" as eda_container {
        [python-app]

        folder "EDA Scripts" {
            [EDA.py]
            [main.py]
        }
    }

    folder "output" as output_eda {
        [correlation_plots.png]
    }
}
note right of "EDA Scripts"
    - Reads from [DP_CDR_Data]
end note

package "Docker Compose Setup 1: Flask App" as flaskapp {
    folder "docker-compose.yaml" as flaskapp_compose {
        [Dockerfile flaskapp]
        [docker-compose flaskapp]
        [init_db.sh]
        [streamrequirements.txt]
    }

    node "Flask App Container" as flaskapp_container {
        [flaskapp-app]
        [flaskapp-db]

        folder "Flask App Scripts" {
            [streamingestion.py]
            [database.py]
        }
    }
    note right of "Flask App Scripts"
        - Initializes Docker
        - Builds MySQL instance
        - Runs [streamingestion.py]
        - Initialized by "API Trigger"
    end note

    node "Database Containerized" as database_container {
        folder "Database Tables" {
            [DP_CDR_Data]
            [Processed_Data]
            [Model_Predictions]
            [CDR_Summary]
        }
    }

    cloud "Streaming API Trigger" as stream_api_trigger {
        [REST API]
    }
}

package "Docker Compose Setup 3: PySpark App" as pyspark {
    folder "docker-compose.yml" as pyspark_compose {
        [docker-compose model config]
        [Dockerfile]
        [entrypoint.sh]
    }

    node "PySpark Container" as pyspark_container {
        [pyspark-analysis]
        [pyspark-model]

        folder "PySpark Scripts" {
            [PySparkAnalysis.py]
            [pySparkModel.py]
        }

        folder "Parquet Files" {
            [parquet]
        }
    }
    note right of "PySpark Scripts"
        - Reads from [DP_CDR_Data]
        - Transforms files: stores [Processed_Data]
        - Writes [Model_Predictions]
    end note
}

package "Docker Compose Setup 4: Model Monitoring" as monitoring {
    folder "docker-compose.yaml" as monitoring_compose {
        [docker-compose model monitoring]
        [Dockerfile model monitoring]
    }

    node "Model Monitoring" as monitoring_scripts {
        [monitoring-script]
        folder "Monitoring Scripts" {
            [Model_Monitoring.py]
            [polling_db.py]
        }
    }

    folder "Output" as monitoring_output {
        [performance_plot.png]
        [performance_report.csv]
    }

    cloud "Monitoring API" as monitoring_api {
        [REST API Trigger]
    }
}

database "MySQL" as mysql {
    [CDRDailySummary]
}

' Relationships
[stream_api_trigger] --> [flaskapp-app]
[streamingestion.py] --> [DP_CDR_Data] : writes
[flaskapp_compose] --* [flaskapp_container]
[flaskapp-db] --o [database_container]
[python-app] --> [flaskapp-db] : shared-network
[eda_container] --> [output_eda]
[eda_compose] --* [eda_container]
[monitoring_api] --> [monitoring_scripts]
[monitoring_compose] --* [monitoring_scripts]

[flaskapp-db] <--> [pyspark-analysis] : shared-network
[PySparkAnalysis.py] --> [Processed_Data] : writes
[PySparkAnalysis.py] --> [parquet] : stores
[pyspark-analysis] --> [PySpark Scripts]
[pySparkModel.py] --> [Model_Predictions] : writes
[pyspark-model] -- [pyspark-analysis] : shared network
[Processed_Data] --> [pySparkModel.py] : reads

[monitoring_scripts] --> [monitoring_output]
[database_container] --> [mysql]
[CDRDailySummary] --> [CDR_Summary] : stores
[flaskapp-db] --> [monitoring_compose] : shared-network
[monitoring_scripts] --> [Model_Predictions] : reads
@enduml
